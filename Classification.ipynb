{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c2081d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2274aa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Race</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>16.60</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>55-59</td>\n",
       "      <td>White</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>20.34</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>80 or older</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>26.58</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Male</td>\n",
       "      <td>65-69</td>\n",
       "      <td>White</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fair</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>24.21</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>75-79</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>23.71</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>40-44</td>\n",
       "      <td>White</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
       "0           No  16.60     Yes              No     No             3.0   \n",
       "1           No  20.34      No              No    Yes             0.0   \n",
       "2           No  26.58     Yes              No     No            20.0   \n",
       "3           No  24.21      No              No     No             0.0   \n",
       "4           No  23.71      No              No     No            28.0   \n",
       "\n",
       "   MentalHealth DiffWalking     Sex  AgeCategory   Race Diabetic  \\\n",
       "0          30.0          No  Female        55-59  White      Yes   \n",
       "1           0.0          No  Female  80 or older  White       No   \n",
       "2          30.0          No    Male        65-69  White      Yes   \n",
       "3           0.0          No  Female        75-79  White       No   \n",
       "4           0.0         Yes  Female        40-44  White       No   \n",
       "\n",
       "  PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
       "0              Yes  Very good        5.0    Yes            No        Yes  \n",
       "1              Yes  Very good        7.0     No            No         No  \n",
       "2              Yes       Fair        8.0    Yes            No         No  \n",
       "3               No       Good        6.0     No            No        Yes  \n",
       "4              Yes  Very good        8.0     No            No         No  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart = pd.read_csv('/home/tiago/Downloads/heart_2020_cleaned.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2305f2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholDrinking</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>DiffWalking</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>Race</th>\n",
       "      <th>Diabetic</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>GenHealth</th>\n",
       "      <th>SleepTime</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>KidneyDisease</th>\n",
       "      <th>SkinCancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>23.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HeartDisease    BMI  Smoking  AlcoholDrinking  Stroke  PhysicalHealth  \\\n",
       "0             0  16.60        1                0       0             3.0   \n",
       "1             0  20.34        0                0       1             0.0   \n",
       "2             0  26.58        1                0       0            20.0   \n",
       "3             0  24.21        0                0       0             0.0   \n",
       "4             0  23.71        0                0       0            28.0   \n",
       "\n",
       "   MentalHealth  DiffWalking  Sex  AgeCategory  Race  Diabetic  \\\n",
       "0          30.0            0    0            7     5         2   \n",
       "1           0.0            0    0           12     5         0   \n",
       "2          30.0            0    1            9     5         2   \n",
       "3           0.0            0    0           11     5         0   \n",
       "4           0.0            1    0            4     5         0   \n",
       "\n",
       "   PhysicalActivity  GenHealth  SleepTime  Asthma  KidneyDisease  SkinCancer  \n",
       "0                 1          4        5.0       1              0           1  \n",
       "1                 1          4        7.0       0              0           0  \n",
       "2                 1          1        8.0       1              0           0  \n",
       "3                 0          2        6.0       0              0           1  \n",
       "4                 1          4        8.0       0              0           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'species'.\n",
    "heart['HeartDisease']= label_encoder.fit_transform(heart['HeartDisease'])\n",
    "heart['Smoking']= label_encoder.fit_transform(heart['Smoking'])\n",
    "heart['AlcoholDrinking']= label_encoder.fit_transform(heart['AlcoholDrinking'])\n",
    "heart['Stroke']= label_encoder.fit_transform(heart['Stroke'])\n",
    "heart['DiffWalking']= label_encoder.fit_transform(heart['DiffWalking'])\n",
    "heart['Sex']= label_encoder.fit_transform(heart['Sex'])\n",
    "heart['AgeCategory']= label_encoder.fit_transform(heart['AgeCategory'])\n",
    "heart['Race']= label_encoder.fit_transform(heart['Race'])\n",
    "heart['Diabetic']= label_encoder.fit_transform(heart['Diabetic'])\n",
    "heart['PhysicalActivity']= label_encoder.fit_transform(heart['PhysicalActivity'])\n",
    "heart['GenHealth']= label_encoder.fit_transform(heart['GenHealth'])\n",
    "heart['Asthma']= label_encoder.fit_transform(heart['Asthma'])\n",
    "heart['KidneyDisease']= label_encoder.fit_transform(heart['KidneyDisease'])\n",
    "heart['SkinCancer']= label_encoder.fit_transform(heart['SkinCancer'])\n",
    "\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753c89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = heart.loc[:, heart.columns != 'HeartDisease'], heart['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62bec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc9f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a99bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:18:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:18:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:18:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:18:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:18:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train score: 0.9147532342219998\n",
      "Test score: 0.9135909275685592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:18:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9141016687686968"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost with Bagging and Boosting\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=xgb_cl, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1422c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9104156243299264\n",
      "Test score: 0.9093069554612827\n",
      "Accuracy: 0.9135909275685592\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier with Bagging and Boosting\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "# Import Decision Tree Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics \n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=23)\n",
    "\n",
    "# The baggging ensemble classifier is initialized with:\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=5, max_samples=50, bootstrap=True)\n",
    "\n",
    "# Training\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "print(f\"Train score: {bagging.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {bagging.score(X_test, y_test)}\")\n",
    "\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "tree = tree.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e44ea261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8435672667007161\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f40141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9137055837563451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print (\"Accuracy : \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f66ddaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.3469 - accuracy: 0.9085 - val_loss: 0.2556 - val_accuracy: 0.9132\n",
      "Epoch 2/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2439 - accuracy: 0.9150 - val_loss: 0.2433 - val_accuracy: 0.9133\n",
      "Epoch 3/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2367 - accuracy: 0.9152 - val_loss: 0.2391 - val_accuracy: 0.9133\n",
      "Epoch 4/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2336 - accuracy: 0.9153 - val_loss: 0.2367 - val_accuracy: 0.9133\n",
      "Epoch 5/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2314 - accuracy: 0.9157 - val_loss: 0.2353 - val_accuracy: 0.9133\n",
      "Epoch 6/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2300 - accuracy: 0.9158 - val_loss: 0.2339 - val_accuracy: 0.9139\n",
      "Epoch 7/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2290 - accuracy: 0.9159 - val_loss: 0.2333 - val_accuracy: 0.9143\n",
      "Epoch 8/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2283 - accuracy: 0.9157 - val_loss: 0.2330 - val_accuracy: 0.9142\n",
      "Epoch 9/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2279 - accuracy: 0.9160 - val_loss: 0.2325 - val_accuracy: 0.9146\n",
      "Epoch 10/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2275 - accuracy: 0.9160 - val_loss: 0.2326 - val_accuracy: 0.9146\n",
      "Epoch 11/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2273 - accuracy: 0.9162 - val_loss: 0.2327 - val_accuracy: 0.9145\n",
      "Epoch 12/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2271 - accuracy: 0.9161 - val_loss: 0.2321 - val_accuracy: 0.9151\n",
      "Epoch 13/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2269 - accuracy: 0.9161 - val_loss: 0.2322 - val_accuracy: 0.9148\n",
      "Epoch 14/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2267 - accuracy: 0.9161 - val_loss: 0.2317 - val_accuracy: 0.9146\n",
      "Epoch 15/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2266 - accuracy: 0.9161 - val_loss: 0.2317 - val_accuracy: 0.9149\n",
      "Epoch 16/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2265 - accuracy: 0.9162 - val_loss: 0.2316 - val_accuracy: 0.9150\n",
      "Epoch 17/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2264 - accuracy: 0.9162 - val_loss: 0.2316 - val_accuracy: 0.9150\n",
      "Epoch 18/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2263 - accuracy: 0.9162 - val_loss: 0.2314 - val_accuracy: 0.9147\n",
      "Epoch 19/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2263 - accuracy: 0.9161 - val_loss: 0.2312 - val_accuracy: 0.9147\n",
      "Epoch 20/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2262 - accuracy: 0.9162 - val_loss: 0.2314 - val_accuracy: 0.9144\n",
      "Epoch 21/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2261 - accuracy: 0.9163 - val_loss: 0.2312 - val_accuracy: 0.9146\n",
      "Epoch 22/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2260 - accuracy: 0.9163 - val_loss: 0.2310 - val_accuracy: 0.9148\n",
      "Epoch 23/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2260 - accuracy: 0.9163 - val_loss: 0.2312 - val_accuracy: 0.9145\n",
      "Epoch 24/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2259 - accuracy: 0.9163 - val_loss: 0.2312 - val_accuracy: 0.9149\n",
      "Epoch 25/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2258 - accuracy: 0.9163 - val_loss: 0.2308 - val_accuracy: 0.9148\n",
      "Epoch 26/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2257 - accuracy: 0.9164 - val_loss: 0.2312 - val_accuracy: 0.9147\n",
      "Epoch 27/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9163 - val_loss: 0.2308 - val_accuracy: 0.9146\n",
      "Epoch 28/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9161 - val_loss: 0.2312 - val_accuracy: 0.9146\n",
      "Epoch 29/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2255 - accuracy: 0.9162 - val_loss: 0.2307 - val_accuracy: 0.9148\n",
      "Epoch 30/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2255 - accuracy: 0.9163 - val_loss: 0.2306 - val_accuracy: 0.9149\n",
      "Epoch 31/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2253 - accuracy: 0.9164 - val_loss: 0.2305 - val_accuracy: 0.9148\n",
      "Epoch 32/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2253 - accuracy: 0.9163 - val_loss: 0.2302 - val_accuracy: 0.9151\n",
      "Epoch 33/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2253 - accuracy: 0.9163 - val_loss: 0.2303 - val_accuracy: 0.9149\n",
      "Epoch 34/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2251 - accuracy: 0.9163 - val_loss: 0.2305 - val_accuracy: 0.9146\n",
      "Epoch 35/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2251 - accuracy: 0.9166 - val_loss: 0.2304 - val_accuracy: 0.9145\n",
      "Epoch 36/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2250 - accuracy: 0.9164 - val_loss: 0.2299 - val_accuracy: 0.9148\n",
      "Epoch 37/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2249 - accuracy: 0.9162 - val_loss: 0.2300 - val_accuracy: 0.9145\n",
      "Epoch 38/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2249 - accuracy: 0.9164 - val_loss: 0.2302 - val_accuracy: 0.9145\n",
      "Epoch 39/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2249 - accuracy: 0.9163 - val_loss: 0.2299 - val_accuracy: 0.9145\n",
      "Epoch 40/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2248 - accuracy: 0.9163 - val_loss: 0.2298 - val_accuracy: 0.9143\n",
      "Epoch 41/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2247 - accuracy: 0.9164 - val_loss: 0.2298 - val_accuracy: 0.9145\n",
      "Epoch 42/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2246 - accuracy: 0.9163 - val_loss: 0.2298 - val_accuracy: 0.9145\n",
      "Epoch 43/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2246 - accuracy: 0.9164 - val_loss: 0.2298 - val_accuracy: 0.9145\n",
      "Epoch 44/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2246 - accuracy: 0.9163 - val_loss: 0.2296 - val_accuracy: 0.9144\n",
      "Epoch 45/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2245 - accuracy: 0.9164 - val_loss: 0.2299 - val_accuracy: 0.9142\n",
      "Epoch 46/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9164 - val_loss: 0.2295 - val_accuracy: 0.9151\n",
      "Epoch 47/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2245 - accuracy: 0.9164 - val_loss: 0.2297 - val_accuracy: 0.9145\n",
      "Epoch 48/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9165 - val_loss: 0.2299 - val_accuracy: 0.9141\n",
      "Epoch 49/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9164 - val_loss: 0.2297 - val_accuracy: 0.9141\n",
      "Epoch 50/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9163 - val_loss: 0.2296 - val_accuracy: 0.9145\n",
      "Epoch 51/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2244 - accuracy: 0.9164 - val_loss: 0.2296 - val_accuracy: 0.9146\n",
      "Epoch 52/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9165 - val_loss: 0.2297 - val_accuracy: 0.9143\n",
      "Epoch 53/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2242 - accuracy: 0.9165 - val_loss: 0.2297 - val_accuracy: 0.9144\n",
      "Epoch 54/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2242 - accuracy: 0.9165 - val_loss: 0.2294 - val_accuracy: 0.9151\n",
      "Epoch 55/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9163 - val_loss: 0.2294 - val_accuracy: 0.9144\n",
      "Epoch 56/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2242 - accuracy: 0.9165 - val_loss: 0.2297 - val_accuracy: 0.9144\n",
      "Epoch 57/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2242 - accuracy: 0.9164 - val_loss: 0.2298 - val_accuracy: 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9164 - val_loss: 0.2296 - val_accuracy: 0.9153\n",
      "Epoch 59/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9144\n",
      "Epoch 60/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9164 - val_loss: 0.2296 - val_accuracy: 0.9148\n",
      "Epoch 61/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9165 - val_loss: 0.2296 - val_accuracy: 0.9143\n",
      "Epoch 62/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2294 - val_accuracy: 0.9145\n",
      "Epoch 63/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9148\n",
      "Epoch 64/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9148\n",
      "Epoch 65/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2240 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "Epoch 66/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9164 - val_loss: 0.2297 - val_accuracy: 0.9144\n",
      "Epoch 67/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2296 - val_accuracy: 0.9144\n",
      "Epoch 68/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2296 - val_accuracy: 0.9147\n",
      "Epoch 69/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9165 - val_loss: 0.2295 - val_accuracy: 0.9152\n",
      "Epoch 70/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9166 - val_loss: 0.2296 - val_accuracy: 0.9146\n",
      "Epoch 71/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2295 - val_accuracy: 0.9145\n",
      "Epoch 72/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9165 - val_loss: 0.2295 - val_accuracy: 0.9146\n",
      "Epoch 73/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9145\n",
      "Epoch 74/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9165 - val_loss: 0.2304 - val_accuracy: 0.9141\n",
      "Epoch 75/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9165 - val_loss: 0.2296 - val_accuracy: 0.9145\n",
      "Epoch 76/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9151\n",
      "Epoch 77/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9141\n",
      "Epoch 78/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9149\n",
      "Epoch 79/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2239 - accuracy: 0.9164 - val_loss: 0.2294 - val_accuracy: 0.9143\n",
      "Epoch 80/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9149\n",
      "Epoch 81/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 82/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9164 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "Epoch 83/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9164 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 84/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 85/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9150\n",
      "Epoch 86/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9153\n",
      "Epoch 87/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 88/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9152\n",
      "Epoch 89/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9148\n",
      "Epoch 90/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9165 - val_loss: 0.2290 - val_accuracy: 0.9156\n",
      "Epoch 91/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9151\n",
      "Epoch 92/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 93/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9165 - val_loss: 0.2290 - val_accuracy: 0.9156\n",
      "Epoch 94/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9144\n",
      "Epoch 95/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9144\n",
      "Epoch 96/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9152\n",
      "Epoch 97/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9164 - val_loss: 0.2293 - val_accuracy: 0.9148\n",
      "Epoch 98/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9165 - val_loss: 0.2294 - val_accuracy: 0.9149\n",
      "Epoch 99/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9149\n",
      "Epoch 100/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 101/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9164 - val_loss: 0.2293 - val_accuracy: 0.9147\n",
      "Epoch 102/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9155\n",
      "Epoch 103/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9150\n",
      "Epoch 104/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9165 - val_loss: 0.2298 - val_accuracy: 0.9145\n",
      "Epoch 105/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9156\n",
      "Epoch 106/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9150\n",
      "Epoch 107/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9145\n",
      "Epoch 108/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9154\n",
      "Epoch 109/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9152\n",
      "Epoch 110/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2296 - val_accuracy: 0.9146\n",
      "Epoch 111/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9155\n",
      "Epoch 112/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 113/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9149\n",
      "Epoch 114/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2296 - val_accuracy: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2236 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 116/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9152\n",
      "Epoch 117/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2290 - val_accuracy: 0.9153\n",
      "Epoch 118/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9157\n",
      "Epoch 119/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9150\n",
      "Epoch 120/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2290 - val_accuracy: 0.9149\n",
      "Epoch 121/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2291 - val_accuracy: 0.9154\n",
      "Epoch 122/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9150\n",
      "Epoch 123/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2288 - val_accuracy: 0.9156\n",
      "Epoch 124/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 125/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2288 - val_accuracy: 0.9155\n",
      "Epoch 126/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 127/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9145\n",
      "Epoch 128/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9165 - val_loss: 0.2293 - val_accuracy: 0.9152\n",
      "Epoch 129/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9165 - val_loss: 0.2291 - val_accuracy: 0.9156\n",
      "Epoch 130/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2235 - accuracy: 0.9164 - val_loss: 0.2289 - val_accuracy: 0.9150\n",
      "Epoch 131/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9168 - val_loss: 0.2288 - val_accuracy: 0.9152\n",
      "Epoch 132/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9155\n",
      "Epoch 133/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9165 - val_loss: 0.2288 - val_accuracy: 0.9153\n",
      "Epoch 134/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 135/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 136/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9153\n",
      "Epoch 137/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9165 - val_loss: 0.2288 - val_accuracy: 0.9153\n",
      "Epoch 138/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 139/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 140/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9151\n",
      "Epoch 141/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 142/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9148\n",
      "Epoch 143/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 144/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 145/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9154\n",
      "Epoch 146/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9156\n",
      "Epoch 147/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 148/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9155\n",
      "Epoch 149/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9151\n",
      "Epoch 150/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9154\n",
      "Epoch 151/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 152/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2295 - val_accuracy: 0.9146\n",
      "Epoch 153/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2298 - val_accuracy: 0.9149\n",
      "Epoch 154/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9149\n",
      "Epoch 155/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "Epoch 156/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9152\n",
      "Epoch 157/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9152\n",
      "Epoch 158/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 159/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2299 - val_accuracy: 0.9149\n",
      "Epoch 160/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2289 - val_accuracy: 0.9153\n",
      "Epoch 161/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 162/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9168 - val_loss: 0.2290 - val_accuracy: 0.9148\n",
      "Epoch 163/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 164/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2289 - val_accuracy: 0.9153\n",
      "Epoch 165/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2233 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9154\n",
      "Epoch 166/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9150\n",
      "Epoch 167/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2288 - val_accuracy: 0.9152\n",
      "Epoch 168/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2295 - val_accuracy: 0.9146\n",
      "Epoch 169/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2296 - val_accuracy: 0.9151\n",
      "Epoch 170/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9155\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2289 - val_accuracy: 0.9152\n",
      "Epoch 172/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2297 - val_accuracy: 0.9149\n",
      "Epoch 173/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2294 - val_accuracy: 0.9149\n",
      "Epoch 174/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9168 - val_loss: 0.2294 - val_accuracy: 0.9154\n",
      "Epoch 175/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9150\n",
      "Epoch 176/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9144\n",
      "Epoch 177/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 178/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 179/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 180/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9150\n",
      "Epoch 181/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2296 - val_accuracy: 0.9150\n",
      "Epoch 182/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.2297 - val_accuracy: 0.9147\n",
      "Epoch 183/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 184/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9145\n",
      "Epoch 185/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9154\n",
      "Epoch 186/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 187/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9152\n",
      "Epoch 188/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9148\n",
      "Epoch 189/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2233 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 190/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9150\n",
      "Epoch 191/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9148\n",
      "Epoch 192/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9150\n",
      "Epoch 193/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 194/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 195/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9153\n",
      "Epoch 196/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9150\n",
      "Epoch 197/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9154\n",
      "Epoch 198/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9156\n",
      "Epoch 199/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9156\n",
      "Epoch 200/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2290 - val_accuracy: 0.9151\n",
      "Epoch 201/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9154\n",
      "Epoch 202/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2290 - val_accuracy: 0.9152\n",
      "Epoch 203/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2300 - val_accuracy: 0.9145\n",
      "Epoch 204/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 205/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 206/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9155\n",
      "Epoch 207/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2295 - val_accuracy: 0.9148\n",
      "Epoch 208/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 209/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9152\n",
      "Epoch 210/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 211/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9164 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "Epoch 212/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "Epoch 213/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2295 - val_accuracy: 0.9149\n",
      "Epoch 214/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2289 - val_accuracy: 0.9152\n",
      "Epoch 215/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 216/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9154\n",
      "Epoch 217/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9151\n",
      "Epoch 218/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9145\n",
      "Epoch 219/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9148\n",
      "Epoch 220/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 221/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 222/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 223/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9150\n",
      "Epoch 224/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9153\n",
      "Epoch 225/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9146\n",
      "Epoch 226/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2294 - val_accuracy: 0.9146\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2290 - val_accuracy: 0.9155\n",
      "Epoch 228/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2296 - val_accuracy: 0.9151\n",
      "Epoch 229/300\n",
      "202/202 [==============================] - 1s 4ms/step - loss: 0.2231 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9155\n",
      "Epoch 230/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 231/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9155\n",
      "Epoch 232/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9151\n",
      "Epoch 233/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9154\n",
      "Epoch 234/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 235/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2290 - val_accuracy: 0.9149\n",
      "Epoch 236/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9169 - val_loss: 0.2293 - val_accuracy: 0.9153\n",
      "Epoch 237/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9166 - val_loss: 0.2295 - val_accuracy: 0.9148\n",
      "Epoch 238/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2297 - val_accuracy: 0.9153\n",
      "Epoch 239/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 240/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2294 - val_accuracy: 0.9149\n",
      "Epoch 241/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9150\n",
      "Epoch 242/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9152\n",
      "Epoch 243/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 244/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9165 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 245/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 246/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9149\n",
      "Epoch 247/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9154\n",
      "Epoch 248/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9154\n",
      "Epoch 249/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9146\n",
      "Epoch 250/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2295 - val_accuracy: 0.9150\n",
      "Epoch 251/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 252/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2300 - val_accuracy: 0.9149\n",
      "Epoch 253/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9167 - val_loss: 0.2296 - val_accuracy: 0.9146\n",
      "Epoch 254/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 255/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9151\n",
      "Epoch 256/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2296 - val_accuracy: 0.9148\n",
      "Epoch 257/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9153\n",
      "Epoch 258/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2298 - val_accuracy: 0.9144\n",
      "Epoch 259/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 260/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 261/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 262/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9147\n",
      "Epoch 263/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2289 - val_accuracy: 0.9156\n",
      "Epoch 264/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9148\n",
      "Epoch 265/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 266/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9157\n",
      "Epoch 267/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9150\n",
      "Epoch 268/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9148\n",
      "Epoch 269/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9154\n",
      "Epoch 270/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9152\n",
      "Epoch 271/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9154\n",
      "Epoch 272/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2291 - val_accuracy: 0.9151\n",
      "Epoch 273/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 274/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9168 - val_loss: 0.2298 - val_accuracy: 0.9145\n",
      "Epoch 275/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2293 - val_accuracy: 0.9149\n",
      "Epoch 276/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9166 - val_loss: 0.2292 - val_accuracy: 0.9148\n",
      "Epoch 277/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9155\n",
      "Epoch 278/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9155\n",
      "Epoch 279/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2298 - val_accuracy: 0.9148\n",
      "Epoch 280/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9151\n",
      "Epoch 281/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9152\n",
      "Epoch 282/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2292 - val_accuracy: 0.9153\n",
      "Epoch 283/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9168 - val_loss: 0.2295 - val_accuracy: 0.9147\n",
      "Epoch 284/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9149\n",
      "Epoch 285/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2294 - val_accuracy: 0.9146\n",
      "Epoch 286/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9168 - val_loss: 0.2296 - val_accuracy: 0.9149\n",
      "Epoch 287/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9165 - val_loss: 0.2295 - val_accuracy: 0.9149\n",
      "Epoch 288/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2299 - val_accuracy: 0.9145\n",
      "Epoch 289/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9155\n",
      "Epoch 290/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2290 - val_accuracy: 0.9155\n",
      "Epoch 291/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9168 - val_loss: 0.2292 - val_accuracy: 0.9149\n",
      "Epoch 292/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9166 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 293/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9169 - val_loss: 0.2291 - val_accuracy: 0.9156\n",
      "Epoch 294/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9158\n",
      "Epoch 295/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2291 - val_accuracy: 0.9153\n",
      "Epoch 296/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2294 - val_accuracy: 0.9152\n",
      "Epoch 297/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9167 - val_loss: 0.2296 - val_accuracy: 0.9146\n",
      "Epoch 298/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9168 - val_loss: 0.2293 - val_accuracy: 0.9152\n",
      "Epoch 299/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2229 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9155\n",
      "Epoch 300/300\n",
      "202/202 [==============================] - 1s 3ms/step - loss: 0.2230 - accuracy: 0.9167 - val_loss: 0.2293 - val_accuracy: 0.9150\n",
      "6996/6996 [==============================] - 11s 2ms/step - loss: 0.2232 - accuracy: 0.9167\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=17, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=1000, validation_split=0.1)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fb27b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy : 0.9141\n",
      "Precision : 0.9916\n",
      "Recall or Sensitivity : 0.9205\n",
      "True Positive Rate : 0.9205\n",
      "False Positive Rate : 0.4839\n",
      "Specificity : 0.5161\n",
      "f1 : 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#metrics XGBoosting\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for data visualization\n",
    "%matplotlib inline\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "# print classification accuracy\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "\n",
    "# print precision score\n",
    "precision = TP / float(TP + FP)\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "\n",
    "recall = TP / float(TP + FN)\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))\n",
    "\n",
    "true_positive_rate = TP / float(TP + FN)\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n",
    "\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "      \n",
    "fone = (2*precision*recall)/(precision+recall)\n",
    "print('f1 : {0:0.4f}'.format(fone))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "801591ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy : 0.9151\n",
      "Precision : 0.9945\n",
      "Recall or Sensitivity : 0.9192\n",
      "True Positive Rate : 0.9192\n",
      "False Positive Rate : 0.4358\n",
      "Specificity : 0.5642\n",
      "f1 : 0.9554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#metrics NN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # for data visualization purposes\n",
    "import seaborn as sns # for data visualization\n",
    "%matplotlib inline\n",
    "\n",
    "y_pred=predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "# print classification accuracy\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "\n",
    "# print precision score\n",
    "precision = TP / float(TP + FP)\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "\n",
    "recall = TP / float(TP + FN)\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))\n",
    "\n",
    "true_positive_rate = TP / float(TP + FN)\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n",
    "\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "      \n",
    "fone = (2*precision*recall)/(precision+recall)\n",
    "print('f1 : {0:0.4f}'.format(fone))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
